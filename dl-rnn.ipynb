{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nclear = lambda: os.system('clear')\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-20T21:39:13.706828Z","iopub.execute_input":"2023-05-20T21:39:13.707422Z","iopub.status.idle":"2023-05-20T21:39:13.982627Z","shell.execute_reply.started":"2023-05-20T21:39:13.707379Z","shell.execute_reply":"2023-05-20T21:39:13.981478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ALL NECESSARY IMPORTS","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader \n\nfrom tqdm import tqdm\nimport heapq\nimport csv\n\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.font_manager import FontProperties\n\nimport wandb\n# Instantiates the device to be used as GPU/CPU based on availability\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n","metadata":{"execution":{"iopub.status.busy":"2023-05-20T21:39:13.985053Z","iopub.execute_input":"2023-05-20T21:39:13.985801Z","iopub.status.idle":"2023-05-20T21:39:20.616673Z","shell.execute_reply.started":"2023-05-20T21:39:13.985760Z","shell.execute_reply":"2023-05-20T21:39:20.615606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Wandb key\nKEY = 'c425b887e2c725018a7f3a772582610fa54ef52c'","metadata":{"execution":{"iopub.status.busy":"2023-05-20T21:39:20.618993Z","iopub.execute_input":"2023-05-20T21:39:20.619777Z","iopub.status.idle":"2023-05-20T21:39:20.624609Z","shell.execute_reply.started":"2023-05-20T21:39:20.619732Z","shell.execute_reply":"2023-05-20T21:39:20.623573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#specify max length of sequence\nmal_embedd_size = 29\neng_embedd_size = 32\ndevice.type","metadata":{"execution":{"iopub.status.busy":"2023-05-20T21:39:24.142697Z","iopub.execute_input":"2023-05-20T21:39:24.143344Z","iopub.status.idle":"2023-05-20T21:39:24.153497Z","shell.execute_reply.started":"2023-05-20T21:39:24.143282Z","shell.execute_reply":"2023-05-20T21:39:24.152111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## PREPROCESSING STEP","metadata":{}},{"cell_type":"code","source":"# Load Data to capture all characters\narr = np.loadtxt(\"/kaggle/input/aksharantar/aksharantar_sampled/mal/mal_train.csv\",\n                 delimiter=\",\", dtype=str)\nnum_sample = arr.shape[0]\nx_train,y_train = arr[:,0],arr[:,1]\nenglish_index = 3\nmal_index = 3\nenglish_dict = {}\nmalayalam_dict = {}\nenglish_index_dict = {}\nmalayalam_index_dict = {}\n\n# Create dictionary for malayalam and english also index dictionary.\n\nfor i in range(num_sample):\n    for j in range(len(x_train[i])):\n        \n        if(english_dict.get(x_train[i][j]) == None):\n            english_dict[x_train[i][j]]=english_index\n            english_index_dict[english_index] = x_train[i][j]\n            english_index+=1\n        \n    for j in range(len(y_train[i])):\n            \n        if(malayalam_dict.get(y_train[i][j]) == None):\n            malayalam_dict[y_train[i][j]]=mal_index\n            malayalam_index_dict[mal_index] = y_train[i][j]\n            mal_index+=1","metadata":{"execution":{"iopub.status.busy":"2023-05-20T21:39:26.562138Z","iopub.execute_input":"2023-05-20T21:39:26.562895Z","iopub.status.idle":"2023-05-20T21:39:28.487774Z","shell.execute_reply.started":"2023-05-20T21:39:26.562837Z","shell.execute_reply":"2023-05-20T21:39:28.486655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Adding start, stop and padding symbols\nmalayalam_index_dict[1] = '<S>'\nenglish_index_dict[1] = '<S>'\n\nmalayalam_index_dict[2] = '<E>'\nenglish_index_dict[2] = '<E>'\n\nmalayalam_index_dict[0] = '<P>'\nenglish_index_dict[0] = '<P>'","metadata":{"execution":{"iopub.status.busy":"2023-05-20T21:39:28.490214Z","iopub.execute_input":"2023-05-20T21:39:28.490623Z","iopub.status.idle":"2023-05-20T21:39:28.499373Z","shell.execute_reply.started":"2023-05-20T21:39:28.490582Z","shell.execute_reply":"2023-05-20T21:39:28.498265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to load data from the data set and create and array corresponding to it\n\ndef loadData(PATH = \"/kaggle/input/aksharantar/aksharantar_sampled/mal/mal_test.csv\"):  \n    \n    arr = np.loadtxt(PATH,\n                 delimiter=\",\", dtype=str)\n    num_sample = arr.shape[0]\n    x,y = arr[:,0],arr[:,1]\n    X = np.zeros((num_sample,eng_embedd_size)) # input\n    Y = np.zeros((num_sample,mal_embedd_size)) # target\n\n    for i in range(num_sample):\n\n        X[i][0] = 1\n        Y[i][0] = 1\n        for j in range(len(x[i])):\n            if(english_dict.get(x[i][j]) != None):\n                X[i][j+1] = english_dict[x[i][j]]\n            else:\n                X[i][j+1] = 0\n\n        X[i][len(x[i])+1]=2\n\n        for j in range(len(y[i])):\n            if(malayalam_dict.get(y[i][j]) != None):\n                Y[i][j+1] = malayalam_dict[y[i][j]]\n            else:\n                Y[i][j+1] = 0\n\n        Y[i][len(y[i])+1] = 2\n        \n    return X, Y","metadata":{"execution":{"iopub.status.busy":"2023-05-20T21:39:28.501003Z","iopub.execute_input":"2023-05-20T21:39:28.502319Z","iopub.status.idle":"2023-05-20T21:39:28.514938Z","shell.execute_reply.started":"2023-05-20T21:39:28.502271Z","shell.execute_reply":"2023-05-20T21:39:28.513872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load all data with specified path\nX_train,y_train = loadData(PATH = \"/kaggle/input/aksharantar/aksharantar_sampled/mal/mal_train.csv\")\nX_val,y_val = loadData(PATH = \"/kaggle/input/aksharantar/aksharantar_sampled/mal/mal_valid.csv\")\nX_test,y_test = loadData(PATH = \"/kaggle/input/aksharantar/aksharantar_sampled/mal/mal_test.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-05-20T21:39:28.517684Z","iopub.execute_input":"2023-05-20T21:39:28.518078Z","iopub.status.idle":"2023-05-20T21:39:33.031632Z","shell.execute_reply.started":"2023-05-20T21:39:28.518039Z","shell.execute_reply":"2023-05-20T21:39:33.030502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Class to create dataset, so can be passed to pytorch dataloader\nclass MakeDataset(Dataset):\n    def __init__(self,x,y):\n        self.x = torch.tensor(x,dtype=torch.int64)\n        self.y = torch.tensor(y,dtype=torch.int64)\n        self.len = x.shape[0]\n\n    def __getitem__(self,idx):\n        return self.x[idx],self.y[idx]\n\n  \n    def __len__(self):\n        return self.len","metadata":{"execution":{"iopub.status.busy":"2023-05-20T21:39:34.106162Z","iopub.execute_input":"2023-05-20T21:39:34.106867Z","iopub.status.idle":"2023-05-20T21:39:34.115466Z","shell.execute_reply.started":"2023-05-20T21:39:34.106828Z","shell.execute_reply":"2023-05-20T21:39:34.113645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create dataset to pass to dataloader\n\ntrain_dataset = MakeDataset(X_train,y_train)\nval_dataset = MakeDataset(X_val, y_val)\ntest_dataset = MakeDataset(X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2023-05-20T21:39:34.547924Z","iopub.execute_input":"2023-05-20T21:39:34.548596Z","iopub.status.idle":"2023-05-20T21:39:34.626510Z","shell.execute_reply.started":"2023-05-20T21:39:34.548559Z","shell.execute_reply":"2023-05-20T21:39:34.625494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create dataloader so getting data in epochs is easy\ntrain_loader = DataLoader(train_dataset,shuffle=True,batch_size=256)\nval_loader = DataLoader(val_dataset,shuffle=True,batch_size=256)\ntest_loader = DataLoader(test_dataset,shuffle=False,batch_size=256)","metadata":{"execution":{"iopub.status.busy":"2023-05-20T21:39:34.948855Z","iopub.execute_input":"2023-05-20T21:39:34.949488Z","iopub.status.idle":"2023-05-20T21:39:34.955856Z","shell.execute_reply.started":"2023-05-20T21:39:34.949452Z","shell.execute_reply":"2023-05-20T21:39:34.954370Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ENCODER MODULE","metadata":{}},{"cell_type":"code","source":"class Encoder(nn.Module):\n    '''\n     Encoder network in which hidden size, input dimension, embedding dimension, etc \n     can be specified for training.\n    '''\n    def __init__(self,\n                    input_dimension = 72,\n                    embed_dimension = 64,\n                    hidden_dimension = 256,\n                    cell_type = 'gru',\n                    layers = 2,\n                    bidirectional = True,\n                    dropout = 0,\n                    device = device\n                ):\n        \n        super(Encoder, self).__init__()\n\n        self.detail_parameters = {\n            'input_dimension' : input_dimension,\n            'embed_dimension' : embed_dimension,\n            'hidden_dimension' : hidden_dimension,\n            'cell_type' : cell_type,\n            'dropout' : dropout,\n            'layers' : layers,\n            'direction_value' : 2 if bidirectional else 1,\n            'device' : device.type,\n        }\n        \n        # total number of english characters\n        self.input_dimension = input_dimension\n        \n        # Dimension to which we need to embed our source\n        self.embed_dimension = embed_dimension\n        \n        # Embedding the input\n        self.embedding = nn.Embedding(self.input_dimension, self.embed_dimension)\n        \n        \n        \n        # Number of neurons in hidden layers\n        self.hidden_dimension = hidden_dimension\n        \n        # Which type to use - RNN, GRU, LSTM\n        self.cell_type = cell_type\n        \n        # Number of layers for hidden \n        self.layers = layers\n        \n        \n        # Dropout to add onto embedded input\n        self.dropout = nn.Dropout(dropout)\n        \n        # If bidirection then hidden size must be doubled.\n        if bidirectional :\n            self.direction_value = 2 \n        else :\n            self.direction_value = 1\n        \n        #device to use gpu or cpu\n        self.device = device\n\n        \n\n        if self.cell_type == 'rnn':\n            # type of cell to use is rnn\n            self.encoder_type = nn.RNN(\n                          input_size= self.embed_dimension,\n                          num_layers= self.layers,\n                          hidden_size= self.hidden_dimension,\n                          dropout = dropout,\n                          bidirectional= bidirectional)\n            \n        elif self.cell_type == \"gru\":\n            # type of cell to use is gru\n            self.encoder_type = nn.GRU(\n                          input_size= self.embed_dimension,\n                          num_layers= self.layers,\n                          hidden_size= self.hidden_dimension,\n                          dropout = dropout,\n                          bidirectional= bidirectional)\n        elif self.cell_type == \"lstm\":\n            # type of cell to use is lstm\n            self.encoder_type = nn.LSTM(\n                          input_size= self.embed_dimension,\n                          num_layers= self.layers,\n                          hidden_size= self.hidden_dimension,\n                          dropout = dropout,\n                          bidirectional= bidirectional)\n            \n            \n    def forward(self, input, hidden, cell=None):\n        \n        # First convert sequence to embedding\n        embedded = self.embedding(input)\n        \n        # Apply dropout also\n        embedded = self.dropout(embedded)\n        \n        #Then choose type of rnn to run using pytorch \n        if self.cell_type == 'lstm':\n            output,(hidden,cell) = self.encoder_type(embedded, (hidden,cell))\n        else:\n            output, hidden = self.encoder_type(embedded, hidden)\n\n        return output, hidden, cell\n\n    def getParams(self):\n        return self.detail_parameters\n    \n    def init_hidden(self, batch):\n        # Initialize the hidden state to zeros\n        return torch.zeros(self.direction_value*self.layers,batch,self.hidden_dimension,device=device)\n        \n    ","metadata":{"execution":{"iopub.status.busy":"2023-05-20T21:39:35.858073Z","iopub.execute_input":"2023-05-20T21:39:35.858472Z","iopub.status.idle":"2023-05-20T21:39:35.878959Z","shell.execute_reply.started":"2023-05-20T21:39:35.858432Z","shell.execute_reply":"2023-05-20T21:39:35.877804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## DECODER MODULE","metadata":{}},{"cell_type":"code","source":"class Decoder(nn.Module):\n    '''\n    Decoder to decode to malayalam word. It also contain different parameters\n    which is specified in the contructor. \n    '''\n    def __init__(self,\n                input_dimension = 26,\n                embed_dimension = 64,\n                hidden_dimension = 256,\n                cell_type = 'lstm',\n                layers = 2,\n                use_attention = False,\n                attention_dimension = None,\n                dropout = 0,\n                bidirectional = True,\n                device = device\n                 ):\n        \n        super(Decoder, self).__init__()\n        \n        self.detail_parameters = {\n            'input_dimension' : input_dimension,\n            'embed_dimension' : embed_dimension,\n            'hidden_dimension' : hidden_dimension,\n            'attention_dimension':attention_dimension,\n            'cell_type' : cell_type,\n            'layers' : layers,\n            'device' : device.type,\n            'dropout' : dropout,\n            'use_attention' : use_attention,\n            'attention_dimension' : attention_dimension,\n        }\n\n        # total number of malayalam characters\n        self.input_dimension = input_dimension\n        \n        # Dimension to which we need to embed our input\n        self.embed_dimension = embed_dimension  \n        \n        # Embedding the input\n        self.embedding = nn.Embedding(self.input_dimension, self.embed_dimension)\n        \n        # Real input size given to nn.Decoder including attention dimension\n        self.input_size = embed_dimension\n        \n        # Attention dimension\n        self.attention_dimension = 0\n        \n        # Number of neurons in hidden layers\n        self.hidden_dimension = hidden_dimension\n        \n        # If attention is being used then set this variable\n        self.use_attention = use_attention\n        \n        # After applying weight, what should be the dimension should be\n        self.attention_out_dimension = 1\n        \n        # Which type to use - RNN, GRU, LSTM\n        self.cell_type = cell_type\n        \n        # Number of layers for hidden\n        self.layers = layers\n        \n        # device to use gpu or cpu\n        self.device = device\n        \n        # Dropout to add onto embedded input\n        self.dropout = nn.Dropout(dropout)\n        \n        \n        \n        if bidirectional :\n            self.direction_value = 2 \n        else :\n            self.direction_value = 1\n            \n        # Weights to multiply output we get from decoder to make it same as input of decoder\n        self.W1 = nn.Linear(self.hidden_dimension*self.direction_value, self.input_dimension)\n\n        \n        self.softmax = F.softmax\n\n        if self.use_attention:\n            self.attention_dimension = attention_dimension\n            self.input_size += self.attention_dimension\n            \n            # Initialize 3 weight matrices, so we can muliply for attention\n            self.U = nn.Sequential(nn.Linear( self.hidden_dimension, self.hidden_dimension), nn.LeakyReLU())\n            self.W = nn.Sequential(nn.Linear( self.hidden_dimension, self.hidden_dimension), nn.LeakyReLU())\n            self.V = nn.Sequential(nn.Linear( self.hidden_dimension, self.attention_out_dimension), nn.LeakyReLU())\n        \n        \n\n        if self.cell_type == 'rnn':\n            # type of cell to use is rnn\n            self.decoder_type = nn.RNN(input_size= self.input_size,\n                            dropout = dropout,     \n                            num_layers= self.layers,\n                            hidden_size= self.hidden_dimension,\n                            bidirectional= bidirectional\n                                 )\n        elif self.cell_type == 'gru':\n            # type of cell to use is gru\n            self.decoder_type = nn.GRU(input_size= self.input_size, # to concat attention_output\n                            num_layers= self.layers,\n                            hidden_size= self.hidden_dimension,\n                            dropout = dropout,\n                            bidirectional= bidirectional\n                                 )\n        elif self.cell_type == \"lstm\":\n            # type of cell to use is lstm\n            self.decoder_type = nn.LSTM(input_size= self.input_size,\n                            num_layers= self.layers,\n                            hidden_size= self.hidden_dimension,\n                            dropout = dropout,\n                            bidirectional= bidirectional\n                                  )\n\n\n    def getParams(self):\n        return self.detail_parameters\n    \n    def applyAttention(self, hidden, enc_output):\n        \n        '''\n        It uses attention mechanism to include encoders weights to decoder.\n        '''\n        encoder_transform  = self.W(enc_output)\n        hidden_transform =  self.U(hidden)\n\n        concat_transform = encoder_transform + hidden_transform\n        score = torch.tanh(concat_transform)\n        \n        score = self.V(score)\n\n        # This will be our probability distribution for the attention weights (alpha)\n        attention_weights = torch.softmax(score, dim=0)\n        \n        # conext vector to be concatenated to input\n        context_vector = attention_weights * enc_output\n        \n        # To make dimension correct after attention\n        normalized_context_vector = torch.sum(context_vector,dim=0)\n        normalized_context_vector = torch.sum(normalized_context_vector,dim=0).unsqueeze(0)\n        \n        return normalized_context_vector,attention_weights\n    \n    def forward(self, input, hidden, cell=None,encoder_outputs=None):\n#         Incorporate dropout in embedding.\n        output = self.dropout(self.embedding(input))\n    \n        attention_weights = None\n#         If we are using attention, then we need to concatenate the context vector, which we obtain from attention\n        \n        if self.use_attention:\n            context,attention_weights = self.applyAttention(hidden, encoder_outputs)\n            output = torch.cat((output,context),2)\n        \n        if self.cell_type == 'lstm':\n            output,(hidden,cell) = self.decoder_type(output,(hidden,cell))\n        else:\n            output, hidden = self.decoder_type(output, hidden)\n            \n        \n        output = self.W1(output)\n        \n        return output, hidden, cell, attention_weights\n\n    ","metadata":{"execution":{"iopub.status.busy":"2023-05-20T21:39:36.626105Z","iopub.execute_input":"2023-05-20T21:39:36.626777Z","iopub.status.idle":"2023-05-20T21:39:36.654698Z","shell.execute_reply.started":"2023-05-20T21:39:36.626737Z","shell.execute_reply":"2023-05-20T21:39:36.653540Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## BEAM SEARCH MODULE","metadata":{}},{"cell_type":"code","source":"class BeamNode(object):\n    '''\n    Beam search uses this class's object as nodes to store all relevent information\n    '''\n    def __init__(self, probability = 1, path_probability = 0, index = 1, hidden = None, cell = None, parent = None):\n        # Probability of this character for predicted\n        self.probability = probability\n        # Total probability of the path\n        self.path_probability = path_probability\n        # index to store, so can be passed as input\n        self.index = index\n        # store the next element\n        self.parent = parent\n        # To store the prev hidden\n        self.hidden = hidden\n        # To store previous cell in case of lstm\n        self.cell = cell\n        # To store path length\n        self.length = 0\n    \n    def __lt__(self, other):\n        # As heapq works on min heap, override less than in a way such that highest pah prob will be taken\n        return self.path_probability > other.path_probability","metadata":{"execution":{"iopub.status.busy":"2023-05-20T21:39:36.893545Z","iopub.execute_input":"2023-05-20T21:39:36.894513Z","iopub.status.idle":"2023-05-20T21:39:36.902798Z","shell.execute_reply.started":"2023-05-20T21:39:36.894465Z","shell.execute_reply":"2023-05-20T21:39:36.901346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BeamSearch():\n    '''\n    Use beam search heuristics to decode. Here it is similar to bfs, but we only choose top k nodes to proceed\n    where k is the beam width.\n    '''\n    def __init__(self, beam_width = 3):\n        \n        # Beam width\n        self.beam_width = beam_width \n        \n        # To store the nodes to be explored\n        self.open_list = []\n        \n        # make it into heap\n        heapq.heapify(self.open_list)\n        \n        # To store all (based on beam width) probable paths\n        self.paths = []\n        \n        \n    def beamSearch(self, model, outputs,dec_hiddens,cells, predicted):\n        batch_size = outputs.shape[1]\n        \n        for i in range(batch_size):\n            with torch.no_grad():\n                model.eval()\n                output = outputs[:,i:i+1].contiguous()\n                index = output.contiguous()\n                dec_hidden = dec_hiddens[:,i:i+1,:].contiguous()\n                cell = cells[:,i:i+1,:].contiguous() if cells is not None else None\n                \n                # create root node => which has 1 (start index) as the element\n                node = BeamNode(1,1,index,dec_hidden, cell, None)\n                heapq.heappush(self.open_list,node)\n\n                # bfs loop\n                while(len(self.open_list) > 0):\n                    curr_node = heapq.heappop(self.open_list)\n                    \n                    if curr_node.length == model.output_seq_length-1:\n                        self.paths.append(curr_node)\n                        continue\n\n                    output,dec_hidden,cell,attention_weights=model.decoder.forward(curr_node.index,curr_node.hidden,curr_node.cell,None)\n                    output = model.softmax(output,dim=2)\n\n                    # take top k  elements from the output\n                    topk, topk_index = torch.topk(output,self.beam_width, dim = 2)\n\n                    # push every neighbour of the curr_node ( bfs neighbours)\n                    for j in range(self.beam_width):\n                        output = topk[:,:,j]\n                        index = topk_index[:,:,j]\n                        \n                        # If prob is less than some threshold, then stop progressing ( to ensure performance )\n                        if curr_node.path_probability * output.item() < 0.001:\n                            continue\n                        node = BeamNode(output.item(),curr_node.path_probability * output.item(),index,dec_hidden, cell, curr_node)\n                        node.length = curr_node.length+1\n                        heapq.heappush(self.open_list,node)\n\n\n                    # Take only k elements to queue in total instead of all nodes, here based on top probabilities\n                    self.open_list = heapq.nsmallest(self.beam_width, self.open_list)\n\n                # out of all the paths explored, take the largest probability path\n                \n            if len(self.paths) > 0:\n                path = min(self.paths)\n                self.paths = []\n\n                # path will be in reversed order, so reversing to make path correct\n                prev = None\n                current = path\n                while(current is not None):\n                    next = current.parent\n                    current.parent = prev\n                    prev = current\n                    current = next\n                path = prev\n\n    #             model.train()\n                # traverse the path according to the path\n                for t in range(1,model.output_seq_length):\n                    output,dec_hidden,cell,attention_weights=model.decoder.forward(path.index,path.hidden,path.cell,None)\n\n                    predicted[t,i:i+1] = output\n\n                    path = path.parent\n            else:\n                output = outputs[:,i:i+1].contiguous()\n                index = output.contiguous()\n                dec_hidden = dec_hiddens[:,i:i+1,:].contiguous()\n                for t in range(1,model.output_seq_length):\n                    output,dec_hidden,cell,attention_weights=model.decoder.forward(index,dec_hidden,cell,None)\n                    predicted[t,i:i+1] = output\n                    output = model.softmax(output,dim=2)\n                    output = torch.argmax(output,dim=2)\n                    \n                ","metadata":{"execution":{"iopub.status.busy":"2023-05-20T21:39:37.162604Z","iopub.execute_input":"2023-05-20T21:39:37.162939Z","iopub.status.idle":"2023-05-20T21:39:37.187572Z","shell.execute_reply.started":"2023-05-20T21:39:37.162907Z","shell.execute_reply":"2023-05-20T21:39:37.186345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scoring function, which is used to calculate how many words in the batch is getting 100% word match\n# This function is used in calculating accuracy\ndef scoring(y_dash , y):\n    num_sample,seq_len = y.shape\n    score = torch.sum(torch.sum(y_dash == y,axis = 1) == seq_len)\n    return score","metadata":{"execution":{"iopub.status.busy":"2023-05-20T21:39:37.420745Z","iopub.execute_input":"2023-05-20T21:39:37.421128Z","iopub.status.idle":"2023-05-20T21:39:37.427721Z","shell.execute_reply.started":"2023-05-20T21:39:37.421093Z","shell.execute_reply":"2023-05-20T21:39:37.426186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SEQUENCE TO SEQUENCE MODULE","metadata":{}},{"cell_type":"code","source":"class Seq2Seq(nn.Module):\n    \n    '''\n    This class incorporate the whole transliteration model. It calls encoder and pass output of encoder\n    to decoder with or wihout attention. Parameters are specified in constructor.\n    '''\n    \n    def __init__(self, \n                 input_seq_length = 32,\n                 output_seq_length = 29,\n                 encoder_input_dimension = 29, \n                 decoder_input_dimension = 72,\n                 encoder_hidden_dimension = 256, \n                 decoder_hidden_dimension =256,\n                 encoder_embed_dimension = 256, \n                 decoder_embed_dimension = 256, \n                 bidirectional = True,\n                 encoder_num_layers = 3,\n                 decoder_num_layers = 2,\n                 cell_type = 'lstm', \n                 dropout = 0.2,\n                 beam_width = 3,\n                 device = device,\n                 attention = False\n                ):\n        \n        \n        super(Seq2Seq, self).__init__()\n        self.detail_parameters = {\n         'input_seq_length': input_seq_length,\n         'output_seq_length' : output_seq_length,\n         'encoder_input_dimension' : encoder_input_dimension, \n         'decoder_input_dimension' : decoder_input_dimension,\n         'encoder_hidden_dimension' : encoder_hidden_dimension,\n         'encoder_embed_dimension' : encoder_embed_dimension, \n         'decoder_hidden_dimension':decoder_hidden_dimension,\n         'decoder_embed_dimension' : decoder_embed_dimension, \n         'bidirectional' : bidirectional,\n         'encoder_num_layers' : encoder_num_layers,\n         'decoder_num_layers' : decoder_num_layers,\n         'cell_type' :cell_type, \n         'dropout' : dropout, \n         'device' : device.type\n        }\n        # Input sequence length => max_length of english\n        self.input_seq_length = input_seq_length\n        \n        # Output sequence length => max_length of malayalam\n        self.output_seq_length = output_seq_length\n        \n        # total number of english characters\n        self.encoder_input_dimension = encoder_input_dimension\n        \n        # total number of malayalam characters\n        self.decoder_input_dimension = decoder_input_dimension\n        \n        # Hidden dim for encoder\n        self.encoder_hidden_dimension = encoder_hidden_dimension\n        \n        # Hidden dim for decoder\n        self.decoder_hidden_dimension = decoder_hidden_dimension\n        \n        # Dimension to which we need to embed our source input\n        self.encoder_embed_dimension = encoder_embed_dimension\n        \n        # Dimension to which we need to embed our target input\n        self.decoder_embed_dimension = decoder_embed_dimension\n        \n        # Whether bidirection needed or not and sets its value as 2, so as to multiply hidden by 2\n        self.direction = bidirectional\n        self.direction_value = 2 if bidirectional else 1\n        \n        # Number of layers for encoder and decoder\n        self.encoder_num_layers = encoder_num_layers\n        self.decoder_num_layers = decoder_num_layers\n        \n        # Which cell type to use\n        self.cell_type = cell_type \n        \n        # Whether to use dropout or not\n        self.dropout = dropout\n        self.device = device\n        \n        self.softmax = F.softmax\n        \n        # fix beam width\n        self.beam_width = beam_width\n        \n        # Whether to use attention or not \n        self.use_attention = attention\n        \n        # Linear Weights so as to make encoder and decoder dimension same (i.e., if they differ by hidden dim or layer)\n        self.enc_dec_linear1 = nn.Linear(encoder_hidden_dimension,decoder_hidden_dimension)\n        self.enc_dec_linear2 = nn.Linear(encoder_num_layers*self.direction_value,decoder_num_layers*self.direction_value)\n        \n        # Linear Weights so as to make encoder and decoder cell's dimension same (i.e., if they differ by hidden dim or layer)\n        self.enc_dec_cell_linear1 = nn.Linear(encoder_hidden_dimension,decoder_hidden_dimension)\n        self.enc_dec_cell_linear2 = nn.Linear(encoder_num_layers*self.direction_value,decoder_num_layers*self.direction_value)\n        \n        # Linear Weights so as to make encoder and decoder attention dimension same (i.e., if they differ by hidden dim or layer)\n        self.enc_dec_att_linear1 = nn.Linear(encoder_hidden_dimension,decoder_hidden_dimension)\n        self.enc_dec_att_linear2 = nn.Linear(encoder_num_layers*self.direction_value,decoder_num_layers*self.direction_value)\n        \n        # initialize encoder\n        self.encoder = Encoder(input_dimension = self.encoder_input_dimension,\n                               embed_dimension = self.encoder_embed_dimension, \n                               hidden_dimension =  self.encoder_hidden_dimension,\n                               cell_type = self.cell_type,\n                               layers = self.encoder_num_layers,\n                               bidirectional = self.direction,\n                               dropout = self.dropout, \n                               device = self.device\n                              )\n        \n        # initialize decoder\n        self.decoder = Decoder(\n                               input_dimension = self.decoder_input_dimension,\n                               embed_dimension = self.decoder_embed_dimension,\n                               hidden_dimension = self.decoder_hidden_dimension,\n                               attention_dimension = self.decoder_hidden_dimension,\n                               cell_type = self.cell_type,\n                               layers = self.decoder_num_layers,\n                               dropout = self.dropout, \n                               device = self.device,\n                                use_attention = self.use_attention\n                               )\n        \n    def getParams(self):\n        return self.detail_parameters\n    \n    def forward(self, input, target ,teacher_force, acc_calculate = False):\n        \n        batch_size = input.shape[0]\n        \n        #initialize hidden dimension o pass to encoder\n        enc_hidden = self.encoder.init_hidden(batch_size)\n        \n        # if lstm then initialize cell also\n        if self.cell_type == 'lstm':\n            cell = self.encoder.init_hidden(batch_size)\n        else:\n            cell = None\n        \n        encoder_outputs = None\n        \n        # if using attention, then encoder outputs should be stored \n        if self.use_attention:\n            encoder_outputs = torch.zeros(self.input_seq_length,self.direction_value*self.decoder_num_layers,batch_size,self.decoder_hidden_dimension,device=device)\n        \n        # Pass input to encoder one by character in batch fashion\n        for t in range(self.input_seq_length):\n            enc_output,enc_hidden, cell = self.encoder.forward(input[:,t].unsqueeze(0), enc_hidden, cell)\n            \n            # Store encoder outputs, by first converting into same dimesnion by linear layers\n            if self.use_attention:\n                enc_hidden_new = enc_hidden\n                enc_hidden_new = self.enc_dec_att_linear1(enc_hidden_new)\n                enc_hidden_new = enc_hidden_new.permute(2,1,0).contiguous()\n                enc_hidden_new = self.enc_dec_att_linear2(enc_hidden_new)\n                enc_hidden_new = enc_hidden_new.permute(2,1,0).contiguous()\n                encoder_outputs[t] = enc_hidden_new\n        \n        # Encoder's last state is decoders first state\n        enc_last_state = enc_hidden\n        \n        # predicted to store all predictions by model to calculate loss\n        predicted = torch.zeros(self.output_seq_length, batch_size, self.decoder_input_dimension,device = self.device)\n        \n        # Store all attention weights, so can be used for plotting attn heatmaps\n        attn_weights = torch.zeros(self.output_seq_length, self.input_seq_length, self.direction_value*self.decoder_num_layers ,batch_size, device = self.device)\n        \n        # Encoders last state is decoders hidden also ransform in case they are of different dimension\n        dec_hidden = enc_last_state\n        dec_hidden = self.enc_dec_linear1(dec_hidden)\n\n        dec_hidden = dec_hidden.permute(2,1,0).contiguous()\n        dec_hidden = self.enc_dec_linear2(dec_hidden)\n        dec_hidden = dec_hidden.permute(2,1,0).contiguous()\n        \n        # Here also, encoders last cell is decoders first cell, also transform to same dimesnion\n        if  self.cell_type == 'lstm':\n            cell = self.enc_dec_cell_linear1(cell)\n            cell = cell.permute(2,1,0).contiguous()\n            cell = self.enc_dec_cell_linear2(cell)\n            cell = cell.permute(2,1,0).contiguous()\n            \n\n        # output at start is all 1's <SOS>\n        output = torch.ones(1,batch_size,dtype=torch.long, device=self.device)\n        predicted[0,:,1]=torch.ones(batch_size)\n        attention_weights = None\n        \n        \n        # Do decoding by char by char fashion by batch   \n        for t in range(1,self.output_seq_length):\n            # if teacher forcing, then pass target directly\n            if teacher_force:\n                output,dec_hidden,cell,attention_weights=self.decoder.forward(target[:,t-1].unsqueeze(0),dec_hidden,cell,encoder_outputs)\n                predicted[t] = output.squeeze(0)\n\n            else:\n                # if beam is to be used, call beam instead of passing output from decoder\n                if self.beam_width > 1 and acc_calculate:\n                    beam = BeamSearch()\n                    beam.beamSearch(self, output,dec_hidden,cell, predicted)\n                    break\n                    \n                # call decoder one at a time\n                output,dec_hidden,cell,attention_weights=self.decoder.forward(output,dec_hidden,cell,encoder_outputs)\n                #store output in prediced (it containes probabilities)\n                predicted[t] = output.squeeze(0)\n                if self.use_attention:\n                    attn_weights[t] = attention_weights.squeeze(3)\n                    \n                # Convert output such that, it can be easily given to input\n                output = self.softmax(output,dim=2)\n                output = torch.argmax(output,dim=2)\n\n        \n        return predicted,attn_weights","metadata":{"execution":{"iopub.status.busy":"2023-05-20T21:39:38.164717Z","iopub.execute_input":"2023-05-20T21:39:38.165084Z","iopub.status.idle":"2023-05-20T21:39:38.211568Z","shell.execute_reply.started":"2023-05-20T21:39:38.165051Z","shell.execute_reply":"2023-05-20T21:39:38.210504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TRAINING FOR 1 EXAMPLE CONFIGURATION","metadata":{}},{"cell_type":"code","source":"def train(data_loader, val_loader ,epochs, beam):\n        \n        # Set all training parameters\n        optimizer = optim.Adam(model.parameters())\n        criterion = nn.CrossEntropyLoss()\n        # set model to train mode\n        model.train()\n        attention_weights = None\n        \n        # Do training in epoch fashion\n        for epoch in tqdm(range(epochs)):\n            total_loss=0\n            train_loss = 0\n            train_score = 0\n            val_score = 0\n            val_loss = 0\n            \n            # use data loader and enumerate each of data for training in batchwise\n            for i, (source, target) in enumerate(data_loader):\n\n                source = source.to(device)\n                target = target.to(device)\n                \n                \n                \n                optimizer.zero_grad()\n                \n                output,attention_weights = model.forward(source, target, epoch < epochs/2, False)                \n\n                # In order to do loss calc, first need to convert target to one-hot and make predicted in probabilistic manner\n                output = output.permute(1, 0, 2)\n                expected = F.one_hot(target,num_classes = 72).float()\n                    \n                # make predicted and target in same dimension\n                output = output.reshape(-1, 72)\n                expected = expected.reshape(-1,72)\n\n                # Calculate loss\n                loss = criterion(output, expected)\n                \n                # Calculate gradients\n                loss.backward()\n                \n                # Clip gradiens, so will not explode\n                nn.utils.clip_grad_norm_(model.parameters(),1)\n                \n                #update parameters\n                optimizer.step()\n                \n #                 break\n#             continue\n\n            # Calculate validation accuracy and losses => Same process as training, but here no updation of gradients\n            with torch.no_grad():\n                model.eval()\n\n                for val_input, val_target in val_loader:\n                    val_input = val_input.to(device)\n                    val_target = val_target.to(device)\n                    val_output,_ = model.forward(val_input, None, False ,False)\n                    \n                    acc_output = F.softmax(val_output,dim=2)\n                    acc_output = torch.argmax(acc_output,dim=2)\n                    acc_output = acc_output.T\n                    val_score += scoring(acc_output,val_target)\n\n                    \n                    val_output = val_output.permute(1, 0, 2)\n                    expected = F.one_hot(val_target,num_classes = 72).float()\n\n                    val_output = val_output.reshape(-1, 72)\n\n                    expected = expected.reshape(-1,72)\n\n                    \n                    loss = criterion(val_output, expected)\n                    val_loss += loss.item()\n              \n            # Calculate training accuracy and losses\n            with torch.no_grad():\n                model.eval()\n                for train_input, train_target in data_loader:\n                    train_input = train_input.to(device)\n                    train_target = train_target.to(device)\n                    train_output,_ = model.forward(train_input, None,False)\n                    \n                    acc_output = F.softmax(train_output,dim=2)\n                    acc_output = torch.argmax(acc_output,dim=2)\n                    acc_output = acc_output.T\n                    train_score += scoring(acc_output,train_target)\n\n                    \n                    train_output = train_output.permute(1, 0, 2)\n                    expected = F.one_hot(train_target,num_classes = 72).float()\n\n                    train_output = train_output.reshape(-1, 72)\n\n                    expected = expected.reshape(-1,72)\n\n                    \n                    loss = criterion(train_output, expected)\n                    train_loss += loss.item()\n                    \n                # Make the model trainable again\n                model.train()\n            \n                \n                \n            print(f'epoch {epoch}')\n            print(f'train loss => {train_loss/len(data_loader)} \\ntrain_acc => {train_score/len(data_loader.dataset)}')\n            print(f'valid loss => {val_loss/len(val_loader)} \\nvalid_acc => {val_score/len(val_loader.dataset)}')\n\nmodel = Seq2Seq(\n    encoder_hidden_dimension = 256, \n    decoder_hidden_dimension =256,\n    encoder_embed_dimension = 256, \n    decoder_embed_dimension = 256, \n    bidirectional = True,\n    encoder_num_layers = 3,\n    decoder_num_layers = 2,\n    cell_type = 'lstm', \n    dropout = 0.2,\n    beam_width = 3,\n    device = device,\n    attention = False\n)\nmodel.to(device)\nepochs = 20\ntrain(train_loader, val_loader, epochs, False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SAVE MODEL","metadata":{}},{"cell_type":"code","source":"# PATH = \"models/model_attention.pt\"\n\n# # Save the model\n# torch.save(model.state_dict(), PATH)","metadata":{"execution":{"iopub.status.busy":"2023-05-19T18:20:35.721920Z","iopub.execute_input":"2023-05-19T18:20:35.722956Z","iopub.status.idle":"2023-05-19T18:20:35.728396Z","shell.execute_reply.started":"2023-05-19T18:20:35.722906Z","shell.execute_reply":"2023-05-19T18:20:35.727203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LOAD MODEL","metadata":{}},{"cell_type":"code","source":"# Load saved model\n# PATH = '/kaggle/input/models/model_attention.pt'\n# model = Seq2Seq()\n# model.load_state_dict(torch.load(PATH))\n# model.to(device)\n# model.eval()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TESTING AND ATTENTION PLOTS","metadata":{}},{"cell_type":"code","source":"# Get test examples, so we can plot the attention part - heatmap\n# test_loader = DataLoader(test_dataset,shuffle=True,batch_size=256)\ntest_input, test_labels = next(iter(test_loader))\nmodel.eval()\n# _weights will have attention weights\ntest_output,_weights = model.forward(test_input.to(device), None,False)","metadata":{"execution":{"iopub.status.busy":"2023-05-18T12:51:37.802096Z","iopub.execute_input":"2023-05-18T12:51:37.802537Z","iopub.status.idle":"2023-05-18T12:51:37.905664Z","shell.execute_reply.started":"2023-05-18T12:51:37.802499Z","shell.execute_reply":"2023-05-18T12:51:37.904371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot heatmaps\n\ndef getTicks(test_input, acc_output, data_index):\n    \n    # put ticks into array by iterating test input and predicted of test and converting it into english-malayalam combination\n    x_t = []\n    y_t = []\n    for i in range(len(test_input[data_index])):\n        if(test_input[data_index][i].item() != 0 and test_input[data_index][i].item() != 1 and test_input[data_index][i].item() != 2):\n            x_t.append(english_index_dict[test_input[data_index][i].item()])\n\n    for i in range(len(acc_output[data_index])):\n        if(acc_output[data_index][i].item() != 0 and acc_output[data_index][i].item() != 1 and acc_output[data_index][i].item() != 2):\n            y_t.append(malayalam_index_dict[acc_output[data_index][i].item()])\n            \n    return x_t, y_t\n\ndef plotHeatMap(test_input,acc_output,w,num_plots = 12):\n    \n    # Create subplots\n    fig, ax = plt.subplots(4, 3,figsize=(20, 20))\n    _ = plt.setp(ax)\n    for data_index in range(num_plots):\n        \n        # get ticks\n        x_t, y_t = getTicks(test_input, acc_output, data_index)\n        \n        # w contains al attention weights for each batches, so take weights batch by batch\n        a = w[:,:,data_index]\n        a = a.detach().cpu().numpy()\n        \n        #remove start and end token's weights \n        a = a[1:len(y_t)+1,2:len(x_t)+2] \n        \n        # plot 3 subplots per row\n        plt.sca(ax[data_index//3,data_index%3])\n        \n        # Heat map using sns library\n#         sns.heatmap(a)\n#         plt.xticks(np.arange(0.5, len(x_t)+0.5), x_t)\n\n        # Anjali dataset to be used as correct font for malayalam in ticks for plot\n#         mal_font = FontProperties(fname = '/kaggle/input/anjali/AnjaliOldLipi-Regular.ttf')\n#         plt.yticks(np.arange(0.5, len(y_t)+0.5), y_t,fontproperties= mal_font)\n        \n        \n        # Heat map using matplotlib\n        plt.imshow(a, interpolation='nearest')\n        plt.colorbar()\n        plt.xticks(np.arange(0, len(x_t)), x_t)\n        \n        # Anjali dataset to be used as correct font for malayalam in ticks for plot\n        mal_font = FontProperties(fname = '/kaggle/input/anjali/AnjaliOldLipi-Regular.ttf')\n        \n        plt.yticks(np.arange(0, len(y_t)), y_t,fontproperties= mal_font)\n        \n        plt.xlabel('English')\n        plt.ylabel('Malayalam')\n        plt.title(f'test image {data_index + 1}')\n        \n        \n#         plt.show()\n    canvas = plt.gca().figure.canvas\n    canvas.draw()\n    data = np.frombuffer(canvas.tostring_rgb(), dtype=np.uint8)\n    # Save image to store it into wandb logs\n    image = data.reshape(canvas.get_width_height()[::-1] + (3,))\n    plt.show()\n    \n    return image\n\n# store it into wandb\n\nwandb.login(key = KEY)\nwandb.init(project = 'dl-assignment-3-final')\n\nif model.use_attention:\n    acc_output = F.softmax(test_output,dim=2)\n    acc_output = torch.argmax(acc_output,dim=2)\n    acc_output.shape\n    acc_output = acc_output.T\n\n    w =  torch.mean(_weights,axis=2)\n    \n    image = plotHeatMap(test_input,acc_output,w)\n    wandb.run.name = 'Attention plots'\n    wandb.log({\"attention_weights_matplotlib\" : [wandb.Image(image,caption=\"Attention weights_matplotlib\")]})\nelse:\n    print(\"No Attention => No heatmap\")\nwandb.finish()\n","metadata":{"execution":{"iopub.status.busy":"2023-05-18T12:56:19.380080Z","iopub.execute_input":"2023-05-18T12:56:19.380474Z","iopub.status.idle":"2023-05-18T12:57:00.877653Z","shell.execute_reply.started":"2023-05-18T12:56:19.380438Z","shell.execute_reply":"2023-05-18T12:57:00.876338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## WANDB RUN FUNCTION","metadata":{}},{"cell_type":"code","source":"# Same as the train loop, but used for wandb running\ndef runModel(model, data_loader, val_loader ,epochs, beam):\n        \n    optimizer = optim.Adam(model.parameters())\n    criterion = nn.CrossEntropyLoss()\n\n    # set model to train mode\n    model.train()\n    train_loss_list = []\n    val_loss_list = []\n    train_accuracy_list = []\n    val_accuracy_list = []\n\n    for epoch in tqdm(range(epochs)):\n        total_loss=0\n        train_loss = 0\n        train_score = 0\n        val_score = 0\n        val_loss = 0\n        for i, (source, target) in enumerate(data_loader):\n\n            source = source.to(device)\n            target = target.to(device)\n\n            optimizer.zero_grad()\n\n            output,_ = model.forward(source, target, epoch < epochs/2, False)                \n\n            output = output.permute(1, 0, 2)\n            expected = F.one_hot(target,num_classes = 72).float()\n\n            output = output.reshape(-1, 72)\n\n            expected = expected.reshape(-1,72)\n\n            loss = criterion(output, expected)\n\n            loss.backward()  # compute gradients\n            nn.utils.clip_grad_norm_(model.parameters(),1)\n            optimizer.step()  # update parameters\n\n            \n        with torch.no_grad():\n            model.eval()\n            for val_input, val_target in val_loader:\n                val_input = val_input.to(device)\n                val_target = val_target.to(device)\n                val_output,_ = model.forward(val_input, None,False,beam)\n\n                acc_output = F.softmax(val_output,dim=2)\n                acc_output = torch.argmax(acc_output,dim=2)\n                acc_output = acc_output.T\n                val_score += scoring(acc_output,val_target)\n\n\n                val_output = val_output.permute(1, 0, 2)\n                expected = F.one_hot(val_target,num_classes = 72).float()\n\n                val_output = val_output.reshape(-1, 72)\n\n                expected = expected.reshape(-1,72)\n\n\n                loss = criterion(val_output, expected)\n                val_loss += loss.item()\n\n        with torch.no_grad():\n            model.eval()\n            for train_input, train_target in data_loader:\n                train_input = train_input.to(device)\n                train_target = train_target.to(device)\n                train_output,_ = model.forward(train_input, None,False,beam)\n\n                acc_output = F.softmax(train_output,dim=2)\n                acc_output = torch.argmax(acc_output,dim=2)\n                acc_output = acc_output.T\n                train_score += scoring(acc_output,train_target)\n\n\n                train_output = train_output.permute(1, 0, 2)\n                expected = F.one_hot(train_target,num_classes = 72).float()\n\n                train_output = train_output.reshape(-1, 72)\n\n                expected = expected.reshape(-1,72)\n\n\n                loss = criterion(train_output, expected)\n                train_loss += loss.item()\n            model.train()\n\n\n\n        print(f'epoch {epoch}')\n        print(f'train loss => {train_loss/len(data_loader)} \\ntrain_acc => {train_score/len(data_loader.dataset)}')\n        print(f'valid loss => {val_loss/len(val_loader)} \\nvalid_acc => {val_score/len(val_loader.dataset)}')\n        train_loss_list.append(train_loss/len(data_loader))\n        val_loss_list.append(val_loss/len(val_loader))\n        train_accuracy_list.append(train_score/len(data_loader.dataset))\n        val_accuracy_list.append(val_score/len(val_loader.dataset))\n\n    return train_loss_list,val_loss_list,train_accuracy_list,val_accuracy_list    \n","metadata":{"execution":{"iopub.status.busy":"2023-05-20T21:44:40.341571Z","iopub.execute_input":"2023-05-20T21:44:40.342018Z","iopub.status.idle":"2023-05-20T21:44:40.365688Z","shell.execute_reply.started":"2023-05-20T21:44:40.341983Z","shell.execute_reply":"2023-05-20T21:44:40.364570Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## RUN SINGLE EXAMPLE AND LOG TO WANDB","metadata":{}},{"cell_type":"code","source":"wandb.login(key = KEY)\nwandb.init(project = 'dl-assignment-3-final')\nmodel = Seq2Seq(\n    encoder_hidden_dimension = 256, \n    decoder_hidden_dimension =256,\n    encoder_embed_dimension = 256, \n    decoder_embed_dimension = 256, \n    bidirectional = True,\n    encoder_num_layers = 3,\n    decoder_num_layers = 3,\n    cell_type = 'lstm', \n    dropout = 0.2,\n    beam_width = 3,\n    device = device,\n    attention = False\n)\n\nbeam = False\nmodel.to(device)\nepochs = 20\ntrain_loss_list,val_loss_list,train_accuracy_list,val_accuracy_list = runModel(model, train_loader, val_loader, epochs, beam)\n\nfor i in range(epochs):\n    wandb.log({'validation_loss': val_loss_list[i],\n              'training_loss': train_loss_list[i],\n              'validation_accuracy': val_accuracy_list[i],\n              'training_accuracy': train_accuracy_list[i]\n              })\n# torch.save(model.state_dict(), PATH = '/kaggle/working/top-model')\nwandb.finish()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## WANDB SWEEPS TRAIN FUNCTION","metadata":{}},{"cell_type":"code","source":"def train_wandb():\n\n    wandb.init(project=\"dl-assignment-3-final\")\n\n    wandb.run.name = f'inp_embed_{wandb.config.input_embedding}_enclayer_{wandb.config.number_of_enc_layer}_declayer_{wandb.config.number_of_dec_layer}_hidden_{wandb.config.hidden_size}_cell_{wandb.config.cell_type}_drop_{wandb.config.dropout}'\n\n    # Give wandb parameters\n    model = Seq2Seq(   \n        encoder_hidden_dimension = wandb.config.hidden_size, \n        decoder_hidden_dimension = wandb.config.hidden_size,\n        encoder_embed_dimension =  wandb.config.input_embedding, \n        decoder_embed_dimension =  wandb.config.input_embedding, \n        bidirectional = wandb.config.bidirectional,\n        encoder_num_layers = wandb.config.number_of_enc_layer,\n        decoder_num_layers = wandb.config.number_of_dec_layer,\n        cell_type = wandb.config.cell_type, \n        dropout = wandb.config.dropout, \n        beam_width = wandb.config.beam_width,\n        device = device,\n        attention = False\n    )\n    model.to(device)\n    beam = True\n    epochs = 20\n    train_loss_list,val_loss_list,train_accuracy_list,val_accuracy_list = runModel(model, train_loader, val_loader, epochs,beam)\n    # Log into wandb\n    for i in range(epochs):\n        wandb.log({'validation_loss': val_loss_list[i],\n                  'training_loss': train_loss_list[i],\n                  'validation_accuracy': val_accuracy_list[i],\n                  'training_accuracy': train_accuracy_list[i]\n                  })\n        \n        ","metadata":{"execution":{"iopub.status.busy":"2023-05-18T13:50:07.256542Z","iopub.execute_input":"2023-05-18T13:50:07.256946Z","iopub.status.idle":"2023-05-18T13:50:07.270881Z","shell.execute_reply.started":"2023-05-18T13:50:07.256909Z","shell.execute_reply":"2023-05-18T13:50:07.269361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## WANDB SWEEP CONFIGURATIONS AND RUN SWEEPS","metadata":{}},{"cell_type":"code","source":"sweep_configuration = {\n    'method': 'bayes',\n    'name': 'dl-assignment-3-final',\n    'metric': {\n        'goal': 'maximize', \n        'name': 'validation_accuracy'\n        },\n    'parameters': {\n        'input_embedding': {'values': [16,32,64,128,256]},\n        'number_of_enc_layer': {'values': [1,2,3]},\n        'number_of_dec_layer': {'values': [1,2,3]},\n        'hidden_size': {'values': [16,32,64,256]},\n        'cell_type': {'values': ['rnn','gru','lstm']},\n        'dropout': {'values': [0.2,0.3]},\n        'bidirectional' : {'values' : [True,False]},\n        'beam_width' : {'values' : [1,3,5]}\n     }\n}\n\nwandb.login(key = KEY)\nsweep_id = wandb.sweep(sweep=sweep_configuration, project='dl-assignment-3-final')\nwandb.agent(sweep_id, function=train_wandb, count=100)\nwandb.finish()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TEST ACCURACY CALCULATIONS","metadata":{}},{"cell_type":"code","source":"# Same as train loop, but calculate accuracy for test\ndef testAccuracy(model, test_loader):\n    model.eval()\n    test_score = 0\n    test_loss = 0\n    criterion = nn.CrossEntropyLoss()\n    \n    for test_input, test_target in test_loader:\n        test_input = test_input.to(device)\n        test_target = test_target.to(device)\n        test_output,_ = model.forward(test_input, None,False,False)\n\n        acc_output = F.softmax(test_output,dim=2)\n        acc_output = torch.argmax(acc_output,dim=2)\n        acc_output = acc_output.T\n        test_score += scoring(acc_output,test_target)\n\n\n        test_output = test_output.permute(1, 0, 2)\n        expected = F.one_hot(test_target,num_classes = 72).float()\n\n        test_output = test_output.reshape(-1, 72)\n\n        expected = expected.reshape(-1,72)\n\n\n        loss = criterion(test_output, expected)\n        test_loss += loss.item()\n        \n    print(f'test loss => {test_loss/len(test_loader)} \\ntest_acc => {test_score/len(test_loader.dataset)}')\n    wandb.log({'test_loss': test_loss/len(test_loader),\n                  'test_accuracy': test_score/len(test_loader.dataset)\n                  })\n\n# Wandb logs for test\n\nwandb.login(key = KEY)\nwandb.init(project = 'dl-assignment-3-final')\nwandb.run.name = f'vannilla_test'\ntestAccuracy(model, test_loader)\nwandb.finish()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## STORE SAMPLE OUTPUT FOR TEST DATA","metadata":{}},{"cell_type":"code","source":"# Function for converting output (malayalam) indexing to native malayalam letters (strings)\ndef printOutput(y):\n    s=''\n    for i in range(len(y)):\n        if y[i] == 0 or y[i] == 1 or y[i] == 2:\n            continue\n        s+=malayalam_index_dict[y[i]]\n    return s\n\n# Function for converting input (english) indexing to native malayalam letters (strings)\ndef printInput(y):\n    s = ''\n    for i in range(len(y)):\n        if y[i] == 0 or y[i] == 1 or y[i] == 2:\n            continue\n        s+=english_index_dict[y[i]]\n    return s\n\n# Run the testing loop, but sore the result by calling above functions and store in csv file as english,predicted_malayalam format\ndef storeSampleOutput(model, test_loader, filename):\n    model.eval()\n    with open(filename, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Input-English','Output-Malayalam','Predicted-Malayalam'])\n        for test_input, test_target in test_loader:\n            test_input = test_input.to(device)\n            test_target = test_target.to(device)\n            test_output,_ = model.forward(test_input, None,False,False)\n\n            acc_output = F.softmax(test_output,dim=2)\n            acc_output = torch.argmax(acc_output,dim=2)\n            acc_output = acc_output.T\n\n            # Create a list of the strings\n            for i in range(len(test_input)):\n                data = [printInput(test_input[i].detach().cpu().numpy()), printOutput(test_target[i].detach().cpu().numpy()) ,printOutput(acc_output[i].detach().cpu().numpy())]\n                \n                # Write in row\n                writer.writerow(data)\n        \n    \nstoreSampleOutput(model, test_loader, 'attention_test_output.csv')  ","metadata":{"execution":{"iopub.status.busy":"2023-05-19T18:20:54.679549Z","iopub.execute_input":"2023-05-19T18:20:54.679956Z","iopub.status.idle":"2023-05-19T18:20:58.080928Z","shell.execute_reply.started":"2023-05-19T18:20:54.679918Z","shell.execute_reply":"2023-05-19T18:20:58.079849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-05-19T18:12:18.844644Z","iopub.execute_input":"2023-05-19T18:12:18.845420Z","iopub.status.idle":"2023-05-19T18:12:18.851136Z","shell.execute_reply.started":"2023-05-19T18:12:18.845377Z","shell.execute_reply":"2023-05-19T18:12:18.849728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}